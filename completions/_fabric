#compdef fabric fabric-ai

_fabric_models() {
  local cache_file="/tmp/fabric-cache/models"

  # Create cache if it doesn't exist
  if [[ ! -f "$cache_file" ]]; then
    mkdir -p /tmp/fabric-cache
    fabric-ai --listmodels --shell-complete-list 2>&1 | grep -v "^OpenAI" > "$cache_file"
  fi

  # Read models from cache
  local -a models
  while IFS= read -r line; do
    [[ -n "$line" ]] && models+=("$line")
  done < "$cache_file"

  # Display completions
  _describe 'available models' models
}

_fabric_patterns() {
  local cache_file="/tmp/fabric-cache/patterns"

  # Create cache if it doesn't exist
  if [[ ! -f "$cache_file" ]]; then
    mkdir -p /tmp/fabric-cache
    fabric-ai --listpatterns --shell-complete-list 2>/dev/null > "$cache_file"
  fi

  # Read patterns from cache
  local -a patterns
  while IFS= read -r line; do
    [[ -n "$line" ]] && patterns+=("$line")
  done < "$cache_file"

  # Display completions
  _describe 'available patterns' patterns
}

_fabric_contexts() {
  local cache_file="/tmp/fabric-cache/contexts"

  if [[ ! -f "$cache_file" ]]; then
    mkdir -p /tmp/fabric-cache
    fabric-ai --listcontexts --shell-complete-list 2>/dev/null > "$cache_file"
  fi

  local -a contexts
  while IFS= read -r line; do
    [[ -n "$line" ]] && contexts+=("$line")
  done < "$cache_file"

  _describe 'available contexts' contexts
}

_fabric_sessions() {
  local cache_file="/tmp/fabric-cache/sessions"

  if [[ ! -f "$cache_file" ]]; then
    mkdir -p /tmp/fabric-cache
    fabric-ai --listsessions --shell-complete-list 2>/dev/null > "$cache_file"
  fi

  local -a sessions
  while IFS= read -r line; do
    [[ -n "$line" ]] && sessions+=("$line")
  done < "$cache_file"

  _describe 'available sessions' sessions
}

_fabric_strategies() {
  local cache_file="/tmp/fabric-cache/strategies"

  if [[ ! -f "$cache_file" ]]; then
    mkdir -p /tmp/fabric-cache
    fabric-ai --liststrategies --shell-complete-list 2>/dev/null > "$cache_file"
  fi

  local -a strategies
  while IFS= read -r line; do
    [[ -n "$line" ]] && strategies+=("$line")
  done < "$cache_file"

  _describe 'available strategies' strategies
}

_fabric() {
  _arguments -s -S \
    '(-p --pattern)'{-p,--pattern}'[Choose a pattern from the available patterns]:pattern:_fabric_patterns' \
    '(-v --variable)'{-v,--variable}'[Values for pattern variables, e.g. -v=#role:expert]:variable:' \
    '(-C --context)'{-C,--context}'[Choose a context from the available contexts]:context:_fabric_contexts' \
    '--session[Choose a session from the available sessions]:session:_fabric_sessions' \
    '(-a --attachment)'{-a,--attachment}'[Attachment path or URL]:file:_files' \
    '(-S --setup)'{-S,--setup}'[Run setup for all reconfigurable parts of fabric]' \
    '(-t --temperature)'{-t,--temperature}'[Set temperature (default: 0.7)]:temperature:' \
    '(-T --topp)'{-T,--topp}'[Set top P (default: 0.9)]:top-p:' \
    '(-s --stream)'{-s,--stream}'[Stream]' \
    '(-P --presencepenalty)'{-P,--presencepenalty}'[Set presence penalty (default: 0.0)]:presence-penalty:' \
    '(-r --raw)'{-r,--raw}'[Use the defaults of the model without sending chat options]' \
    '(-F --frequencypenalty)'{-F,--frequencypenalty}'[Set frequency penalty (default: 0.0)]:frequency-penalty:' \
    '(-l --listpatterns)'{-l,--listpatterns}'[List all patterns]' \
    '(-L --listmodels)'{-L,--listmodels}'[List all available models]' \
    '(-x --listcontexts)'{-x,--listcontexts}'[List all contexts]' \
    '(-X --listsessions)'{-X,--listsessions}'[List all sessions]' \
    '(-U --updatepatterns)'{-U,--updatepatterns}'[Update patterns]' \
    '(-c --copy)'{-c,--copy}'[Copy to clipboard]' \
    '(-m --model)'{-m,--model}'[Choose model]:model:_fabric_models' \
    '--modelContextLength[Model context length (only affects ollama)]:context-length:' \
    '(-o --output)'{-o,--output}'[Output to file]:output-file:_files' \
    '--output-session[Output the entire session to the output file]' \
    '(-n --latest)'{-n,--latest}'[Number of latest patterns to list (default: 0)]:count:' \
    '(-d --changeDefaultModel)'{-d,--changeDefaultModel}'[Change default model]' \
    '(-y --youtube)'{-y,--youtube}'[YouTube video or playlist URL to grab transcript]:url:' \
    '--playlist[Prefer playlist over video if both ids are present in the URL]' \
    '--transcript[Grab transcript from YouTube video and send to chat]' \
    '--transcript-with-timestamps[Grab transcript from YouTube video with timestamps]' \
    '--comments[Grab comments from YouTube video and send to chat]' \
    '--metadata[Output video metadata]' \
    '(-g --language)'{-g,--language}'[Specify the Language Code for the chat]:language:(en es fr de it pt ru ja ko zh ar)' \
    '(-u --scrape_url)'{-u,--scrape_url}'[Scrape website URL to markdown using Jina AI]:url:' \
    '(-q --scrape_question)'{-q,--scrape_question}'[Search question using Jina AI]:question:' \
    '(-e --seed)'{-e,--seed}'[Seed to be used for LLM generation]:seed:' \
    '(-w --wipecontext)'{-w,--wipecontext}'[Wipe context]:context:_fabric_contexts' \
    '(-W --wipesession)'{-W,--wipesession}'[Wipe session]:session:_fabric_sessions' \
    '--printcontext[Print context]:context:_fabric_contexts' \
    '--printsession[Print session]:session:_fabric_sessions' \
    '--readability[Convert HTML input into a clean, readable view]' \
    '--input-has-vars[Apply variables to user input]' \
    '--dry-run[Show what would be sent to the model without actually sending it]' \
    '--serve[Serve the Fabric Rest API]' \
    '--serveOllama[Serve the Fabric Rest API with ollama endpoints]' \
    '--address[The address to bind the REST API (default: :8080)]:address:' \
    '--api-key[API key used to secure server routes]:api-key:' \
    '--config[Path to YAML config file]:config-file:_files' \
    '--version[Print current version]' \
    '--listextensions[List all registered extensions]' \
    '--addextension[Register a new extension from config file path]:extension-file:_files' \
    '--rmextension[Remove a registered extension by name]:extension-name:' \
    '--strategy[Choose a strategy from the available strategies]:strategy:_fabric_strategies' \
    '--liststrategies[List all strategies]' \
    '--listvendors[List all vendors]' \
    '--shell-complete-list[Output raw list without headers/formatting (for shell completion)]' \
    '--search[Enable web search tool for supported models (Anthropic, OpenAI)]' \
    '--search-location[Set location for web search results]:location:' \
    '--image-file[Save generated image to specified file path]:image-file:_files' \
    '--image-size[Image dimensions]:size:(1024x1024 1536x1024 1024x1536 auto)' \
    '--image-quality[Image quality]:quality:(low medium high auto)' \
    '--image-compression[Compression level 0-100 for JPEG/WebP formats]:compression:' \
    '--image-background[Background type]:background:(opaque transparent)' \
    '--suppress-think[Suppress text enclosed in thinking tags]' \
    '--think-start-tag[Start tag for thinking sections (default: <think>)]:tag:' \
    '--think-end-tag[End tag for thinking sections (default: </think>)]:tag:' \
    '--disable-responses-api[Disable OpenAI Responses API (default: false)]' \
    '--voice[TTS voice name for supported models (default: Kore)]:voice:(Kore Charon Puck Aoede Fenrir)' \
    '--list-gemini-voices[List all available Gemini TTS voices]' \
    '(-h --help)'{-h,--help}'[Show this help message]'
}

_fabric "$@"
